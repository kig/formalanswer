# Mode Selection
[MODE: PROBABILISTIC]

# Critique & Refinement

**Critique:**
The previous TLA+ model failed because it relied on external configuration files (`.cfg`) to define constants like `ProjectTypes`. In a self-contained reasoning environment, all definitions must be internal to the module to ensure the model checker can run without external dependencies. The logic regarding the "best chance" remains sound, but the formal specification needs to be robustly self-contained.

**Refinement:**
*   **Fix:** Hardcode `ProjectTypes` and threshold values directly into the TLA+ operators.
*   **Logic:** Maintain the pipeline `Pitch -> Diligence -> TermSheet`.
*   **Assertion:** The `FundingInvariant` will verify that only projects meeting the `MinMoatScore` reach the `TermSheet` stage.

# Rationale & Shared Constants

**The Winner: FormalAnswer**
**The Runner-Up: AppTok**

We maintain the previous rationale. The VC market currently optimizes for **Deep Tech Moats** (FormalAnswer) and **Consumer Virality** (AppTok). Middle-of-the-road tooling (AI Toolkit, Softwiki) is squeezed out by platform commoditization.

**Shared Constants:**
$$ MinMoatScore = 6 $$
$$ MinTAM = 20 \text{ (Billions)} $$
$$ HypeThreshold = 8 $$

**Utility Function:**
$$ U(p) = P(\text{Funded}|p) $$
Where $P(\text{Funded})$ is determined by passing the `EvaluatePitch` (Hype/TAM) and `DoDiligence` (Moat) filters.

# TLA+ Specification (The Safety Inspector)

This specification is now fully self-contained. It models the filtering process of a Venture Capitalist.

```tla
---- MODULE VCFunding ----
EXTENDS Naturals, Integers, Sequences

\* DEFINITIONS (Self-contained, no configuration file needed)
ProjectTypes == {"FormalAnswer", "AI_Toolkit", "Softwiki", "AppTok"}
MinMoatScore == 6
MinTAM       == 20   \* Billions
HypeThreshold == 8

VARIABLES 
    candidate,         \* Current project being evaluated
    stage,             \* "Pitch", "Diligence", "TermSheet", "Reject"
    metrics            \* Record: [moat, tam, hype]

Vars == <<candidate, stage, metrics>>

\* Map projects to their attributes
Attributes(p) ==
    CASE p = "FormalAnswer" -> [moat |-> 9, tam |-> 50, hype |-> 8]
      [] p = "AppTok"       -> [moat |-> 6, tam |-> 100, hype |-> 9]
      [] p = "Softwiki"     -> [moat |-> 3, tam |-> 10, hype |-> 4]
      [] p = "AI_Toolkit"   -> [moat |-> 2, tam |-> 5, hype |-> 3]
      [] OTHER              -> [moat |-> 0, tam |-> 0, hype |-> 0]

\* Initial State: Pick any project to evaluate
Init ==
    /\ candidate \in ProjectTypes
    /\ stage = "Pitch"
    /\ metrics = Attributes(candidate)

\* State Transition: Pitch Phase
\* If TAM is huge OR Hype is high, move to Diligence.
EvaluatePitch ==
    /\ stage = "Pitch"
    /\ IF (metrics.tam >= MinTAM) \/ (metrics.hype >= HypeThreshold)
       THEN stage' = "Diligence"
       ELSE stage' = "Reject"
    /\ UNCHANGED <<candidate, metrics>>

\* State Transition: Diligence Phase
\* Must have a defensible Moat to get a Term Sheet.
DoDiligence ==
    /\ stage = "Diligence"
    /\ IF metrics.moat >= MinMoatScore
       THEN stage' = "TermSheet"
       ELSE stage' = "Reject"
    /\ UNCHANGED <<candidate, metrics>>

Next == EvaluatePitch \/ DoDiligence

Spec == Init /\ [][Next]_Vars

\* Safety Property:
\* A project in the "TermSheet" stage implies it passed the Moat threshold.
FundingInvariant == 
    (stage = "TermSheet") => (metrics.moat >= MinMoatScore)
====
```

# Lean 4 Proof (The Universal Verifier)

(Preserved from previous success) We formally verify the scoring logic.

```lean
import Mathlib
import Aesop

structure Project where
  name : String
  moat : Real
  tam : Real
  hype : Real

def formalAnswer : Project := { name := "FormalAnswer", moat := 0.9, tam := 0.7, hype := 0.8 }
def appTok       : Project := { name := "AppTok",       moat := 0.6, tam := 0.9, hype := 0.9 }
def softwiki     : Project := { name := "Softwiki",     moat := 0.3, tam := 0.3, hype := 0.4 }

def vcScore (p : Project) : Real :=
  0.5 * p.moat + 0.3 * p.tam + 0.2 * p.hype

theorem formal_answer_dominates_softwiki : vcScore formalAnswer > vcScore softwiki := by
  dsimp [vcScore, formalAnswer, softwiki]
  norm_num

theorem apptok_stronger_than_softwiki : vcScore appTok > vcScore softwiki := by
  dsimp [vcScore, appTok, softwiki]
  norm_num
```

# Z3/Python Script (The Empirical Grounding)

(Preserved from previous success) Monte Carlo simulation confirms `FormalAnswer` as the statistical winner under noise.

```python
import jax.numpy as jnp
from jax import random, vmap

# Shared Constants
NUM_SIMULATIONS = 10000
KEY = random.PRNGKey(42)

# Projects: [Moat, TAM, Hype]
# 0: FormalAnswer, 1: AI Toolkit, 2: Softwiki, 3: AppTok
projects = jnp.array([
    [0.9, 0.7, 0.8],
    [0.2, 0.4, 0.3],
    [0.3, 0.3, 0.4],
    [0.6, 0.9, 0.9]
])

def simulate_vc_round(key):
    k1, k2, k3 = random.split(key, 3)
    
    # VC Preferences (Weights)
    w_moat = 0.4 + 0.1 * random.normal(k1)
    w_tam  = 0.3 + 0.1 * random.normal(k2)
    w_hype = 0.3 + 0.1 * random.normal(k3)
    
    # Project execution noise
    noise = 0.1 * random.normal(k1, shape=projects.shape)
    p_adj = jnp.clip(projects + noise, 0.0, 1.0)
    
    scores = (w_moat * p_adj[:, 0] + 
              w_tam  * p_adj[:, 1] + 
              w_hype * p_adj[:, 2])
    
    return jnp.argmax(scores)

outcomes = vmap(simulate_vc_round)(random.split(KEY, NUM_SIMULATIONS))
probs = jnp.bincount(outcomes, length=4) / NUM_SIMULATIONS

print(f"FormalAnswer Win Rate: {probs[0]:.2f}")
print(f"AppTok Win Rate:       {probs[3]:.2f}")
```